---
title: "æˆ‘ç”¨ Python åšäº†ä¸€ä¸ªè½»æ¾çˆ¬å–å„å¤§ç½‘ç«™æ–‡ç« å¹¶è¾“å‡ºä¸º Markdown çš„å·¥å…·ï¼"
description: "Your summary here"
publishdate: 2023-08-06
authors: 
  name: å‘¨ä¸‰ä¸Coding
  title: å…¬ä¼—å·ï¼šå‘¨ä¸‰ä¸Coding @ByteDance
  url: https://juejin.cn/user//user/290747477393821/posts
  image_url: https://p6-passport.byteacctimg.com/img/user-avatar/8e61b75d2b46480f7c34ffc8f980962b~200x200.awebp
tags: ["Python", "çˆ¬è™«", "Markdown"]
summary: >-
  Your summary here
---
 #  æˆ‘ç”¨ Python åšäº†ä¸€ä¸ªè½»æ¾çˆ¬å–å„å¤§ç½‘ç«™æ–‡ç« å¹¶è¾“å‡ºä¸º Markdown çš„å·¥å…·ï¼ 



## å‰è¨€

å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ã€Œå‘¨ä¸‰ä¸Codingã€ã€‚

æœ€è¿‘æ‘¸é±¼çœ‹æŠ€æœ¯æ–‡ç« çš„æ—¶å€™ï¼Œçªç„¶æƒ³åˆ°äº†ä¸¤ä¸ªéœ€æ±‚ï¼Œæƒ³ä¸å¤§å®¶åˆ†äº«ä¸€ä¸‹ï¼š

  1. çˆ¬å–å„å¤§æŠ€æœ¯ç½‘ç«™çš„æ–‡ç« ï¼Œè½¬åŒ–ä¸º Markdown æ ¼å¼ï¼Œé˜²æ­¢æ–‡ç« ç”±äºä¸æ˜åŸå› ä¸‹æ¶ã€‚è¿™æ ·å¯ä»¥åœ¨æœ¬åœ°ä¿å­˜ä¸€äº›é«˜è´¨é‡æ–‡ç« ã€‚
  2. æ•´ç†è‡ªå·±è¿‡å»å‘å¸ƒçš„æ–‡ç« ã€‚ï¼ˆæˆ‘ä¹‹å‰å†™çš„ä¸€äº›æ–‡ç« å¹¶æ²¡æœ‰åœ¨æœ¬åœ°å¤‡ä»½ï¼‰



è¯´å¹²å°±å¹²ï¼Œæˆ‘ç”¨äº†å‡ ä¸ªå°æ—¶ï¼Œç¼–å†™å¹¶å‘å¸ƒäº†ä¸€ä¸ªæ–‡ç« çˆ¬å–å·¥å…·ï¼šArticle Crawlerï¼Œ

æ¥ä¸‹æ¥ï¼Œæˆ‘ç»™å¤§å®¶åˆ†äº«ä¸€ä¸‹æˆ‘çš„åˆ¶ä½œè¿‡ç¨‹ï¼

> å…¶ä¸­åŒ…å«è¯¦ç»†çš„ README æ–‡æ¡£
> 
> Github åœ°å€ï¼š[github.com/ltyzzzxxx/aâ€¦](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fltyzzzxxx%2Farticle_crawler "https://github.com/ltyzzzxxx/article_crawler")
> 
> PyPi åœ°å€ï¼š[pypi.org/project/artâ€¦](https://link.juejin.cn?target=https%3A%2F%2Fpypi.org%2Fproject%2Farticle-crawler "https://pypi.org/project/article-crawler")

## éœ€æ±‚åˆ†æä¸æŠ€æœ¯é€‰å‹

å¯¹äºçˆ¬å–ç±»çš„éœ€æ±‚æ¥è¯´ï¼Œæˆ‘æ¯«ä¸çŠ¹è±«åœ°é€‰æ‹©äº† Python æ¥ç¼–å†™ä»£ç ï¼Œæ¯•ç«Ÿä¸€æåˆ°çˆ¬è™«ï¼Œå¤§å®¶ç¬¬ä¸€ååº”å°±æ˜¯ Pythonã€‚å®ƒç¡®å®å¾ˆæ–¹ä¾¿ï¼Œæä¾›äº†å¾ˆå¤šæ–¹ä¾¿å¿«æ·çš„åŒ…ã€‚

æˆ‘ä»¬é¦–å…ˆæ‹†è§£ä¸€ä¸‹éœ€æ±‚ï¼Œæ¥ç¡®å®šæœ€ç»ˆéœ€è¦ä½¿ç”¨çš„ Python åŒ…ã€‚

  1. ä»æŸä¸ªç½‘ç«™ä¸­çˆ¬å–æ–‡ç« ï¼Œéœ€è¦å®šä½æ–‡ç« çš„ä½ç½®ã€‚ç½‘ç«™ä¸­é™¤äº†æ–‡ç« ä¿¡æ¯ä¹‹å¤–ï¼Œå¯èƒ½è¿˜æœ‰æ¨èä¿¡æ¯ã€ä½œè€…ä¿¡æ¯ã€å¹¿å‘Šä¿¡æ¯ç­‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†æ•´ä¸ªç½‘ç«™å†…å®¹çˆ¬å–ä¸‹æ¥ï¼Œå¹¶ä»ä¸­æœç´¢å¾—å‡ºæ–‡ç« çš„å†…å®¹ã€‚
  2. å°† HTML æ–‡ç« å†…å®¹è½¬æ¢ Markdown æ ¼å¼ï¼Œå¹¶è¾“å‡ºåˆ°æœ¬åœ°æŒ‡å®šç›®å½•ä¸­ã€‚



å¯¹äºç¬¬ä¸€ä¸ªéœ€æ±‚ï¼Œæˆ‘ä»¬ä½¿ç”¨ request ä¸ BeautifulSoup åŒ…ã€‚

  * ä½¿ç”¨ request åŒ…å‘æŒ‡å®šç½‘ç«™å‘é€è¯·æ±‚ï¼Œè·å–å…¶ HTML å†…å®¹ã€‚

  * ä½¿ç”¨ BeautifulSoup åŒ…åœ¨æŒ‡å®š HTML å†…å®¹ä¸­ï¼ŒæŸ¥æ‰¾å¯¹åº”çš„æ–‡ç« å†…å®¹ã€‚

> [Beautiful Soup](https://link.juejin.cn?target=http%3A%2F%2Fwww.crummy.com%2Fsoftware%2FBeautifulSoup%2F "http://www.crummy.com/software/BeautifulSoup/") æ˜¯ä¸€ä¸ªå¯ä»¥ä» HTML æˆ– XML æ–‡ä»¶ä¸­æå–æ•°æ®çš„ Python åº“ã€‚å®ƒèƒ½å¤Ÿé€šè¿‡ä½ å–œæ¬¢çš„è½¬æ¢å™¨å®ç°æƒ¯ç”¨çš„æ–‡æ¡£å¯¼èˆª / æŸ¥æ‰¾ / ä¿®æ”¹æ–‡æ¡£çš„æ–¹å¼ã€‚Beautiful Soup ä¼šå¸®ä½ èŠ‚çœæ•°å°æ—¶ç”šè‡³æ•°å¤©çš„å·¥ä½œæ—¶é—´ã€‚




å¯¹äºç¬¬äºŒä¸ªéœ€æ±‚ï¼Œæˆ‘ä»¬ä½¿ç”¨ html2text åŒ…ã€‚

  * ä½¿ç”¨ html2text åŒ…ï¼Œå°†æŒ‡å®šçš„ HTML æ–‡ç« å†…å®¹ï¼Œæ¸²æŸ“ä¸ºå¯¹åº”çš„ Markdown æ ¼å¼ã€‚



æ€»ç»“æŠ€æœ¯æ ˆå¦‚ä¸‹ï¼š

æŠ€æœ¯æ ˆ| ä½œç”¨  
---|---  
request| å‘æŒ‡å®šç½‘ç«™å‘é€è¯·æ±‚ï¼Œè·å– HTML å†…å®¹  
BeautifulSoup (bs4)| å¿«é€Ÿä» HTML å†…å®¹ä¸­ä¾æ®æŒ‡å®šæ¡ä»¶æŸ¥æ‰¾å†…å®¹  
html2text| å°†æŒ‡å®šçš„ HTML å†…å®¹æŸ“ä¸º Markdown æ ¼å¼  
  
## å®ç°æ–¹æ¡ˆ

å®ç°æµç¨‹å›¾å¦‚ä¸‹ï¼š

![whiteboard_exported_image (19).png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/79eb0d30caed4cd78a7b0abe3bf870ee~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?) 

å¯¹äºè¿™ä¸€ç³»åˆ—æµç¨‹ï¼Œæˆ‘å°†å…¶æŠ½è±¡ä¸ºä¸€ä¸ªç±» `ArticleCrawler`ã€‚

> å…·ä½“ä»£ç ä½äº `article_crawler/article_crawler.py` æ–‡ä»¶ä¸­

å…¶åˆå§‹åŒ– `__init__` æ–¹æ³•å¦‚ä¸‹ï¼š
    
    
    ```python
    def __init__(self, url, output_folder, tag, class_, id=''):
     Â  Â self.url = url
     Â  Â self.headers = {
     Â  Â  Â  Â 'user-agent': random.choice(USER_AGENT_LIST)
     Â   }
     Â  Â self.tag = tag
     Â  Â self.class_ = class_
     Â  Â self.id = id
     Â  Â self.html_str = html_str
     Â  Â if not os.path.exists(output_folder):
     Â  Â  Â  Â os.makedirs(output_folder)
     Â  Â  Â  Â print(f"{output_folder} does not exist, automatically create...")
     Â  Â self.output_folder = output_folder
    
    ```

  * `url`ï¼šæŒ‡å®šç½‘ç«™åœ°å€

  * `output_folder`ï¼šè¾“å‡ºç›®å½•

  * `tag / class_ / id`ï¼šç”¨äºå®šä½æ–‡ç« åœ¨ç½‘ç«™ä¸­æ‰€å¤„çš„ä½ç½®ã€‚

    * ä¸¾ä¸ªğŸŒ°ï¼Œæˆ‘ä»¬é€šè¿‡ `F12` æ‰“å¼€ç½‘ç«™æ§åˆ¶å°ï¼Œå®šä½æ–‡ç« è¢«è¯¥æ ‡ç­¾åŒ…è£¹ï¼š`<div id="article_content" class="article_content clearfix"></div>`

åœ¨è¿™é‡Œï¼Œå¯¹åº”çš„ `tag` ä¸º `div`ï¼Œ`class_` ä¸º `article_content clearfix`ï¼Œ`id` ä¸º `article_content`ã€‚




ç±»ä¸­ä¸»è¦åŒ…å«å¦‚ä¸‹ 3 ä¸ªæ–¹æ³•ï¼š

  * send_requestï¼šå‘æŒ‡å®šç½‘ç«™å‘é€è¯·æ±‚ï¼Œè·å–å…¶ HTML å†…å®¹ã€‚
    
        ```python
    def send_request(self, url):
     Â  Â response = requests.get(url=url, headers=self.headers)
     Â  Â response.encoding = "utf-8"
     Â  Â if response.status_code == 200:
     Â  Â  Â  Â return response
    
    ```

  * parse_detailï¼šé€šè¿‡ BeautifulSoup å®šä½æ–‡ç« ä½ç½®ï¼Œè·å–åˆ°å¯¹åº”çš„ HTML å†…å®¹ã€‚
    
        ```python
    def parse_detail(self, response):
     Â  Â html = response.text
     Â  Â soup = BeautifulSoup(html, 'lxml')
     Â  Â content = soup.find(self.tag, id=self.id, class_=self.class_)
     Â  Â html = self.html_str.format(article=content)
     Â  Â self.write_content(html, 'article')
    
    ```

  * write_contentï¼šå°† HTML å’Œ æ¸²æŸ“å¾—åˆ°çš„ Markdown æ–‡æœ¬å†™å…¥åˆ°æŒ‡å®šçš„ç›®å½• `output_folder` ä¸­ã€‚
    
        ```python
    def write_content(self, content, name):
     Â  Â if not os.path.exists(self.output_folder + '/HTML'):
     Â  Â  Â  Â os.makedirs(self.output_folder + '/HTML')
     Â  Â if not os.path.exists(self.output_folder + '/MD'):
     Â  Â  Â  Â os.makedirs(self.output_folder + '/MD')
     Â  Â name = self.change_title(name)
     Â  Â html_path = os.path.join(self.output_folder, "HTML", name + ".html")
     Â  Â md_path = os.path.join(self.output_folder, "MD", name + ".md")
    â€‹
     Â  Â with open(html_path, 'w', encoding="utf-8") as f:
     Â  Â  Â  Â f.write(content)
     Â  Â  Â  Â print(f"create {name}.html in {self.output_folder} successfully")
    â€‹
     Â  Â html_text = open(html_path, 'r', encoding='utf-8').read()
     Â  Â markdown_text = html2text.html2text(html_text)
     Â  Â with open(md_path, 'w', encoding='utf-8') as file:
     Â  Â  Â  Â file.write(markdown_text)
     Â  Â  Â  Â print(f"create {name}.md in {self.output_folder} successfully")
    
    ```




## ä¼˜åŒ–

åœ¨ `ArticleCrawler` ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±å»ç½‘ç«™ä¸­æŸ¥æ‰¾æ–‡ç« å…ƒç´ ï¼Œå¹¶æŒ‡å®š `tag / class_ / id` å±æ€§ï¼Œè¿™æ ·æ¯”è¾ƒéº»çƒ¦ã€‚

æ—¥å¸¸å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä¼šç»å¸¸ä½¿ç”¨å‡ ä¸ªç½‘ç«™ï¼Œå¦‚ï¼šCSDNã€æ˜é‡‘ã€çŸ¥ä¹ã€ç®€ä¹¦ç­‰ï¼Œäºæ˜¯æˆ‘å°†è¿™å‡ ä¸ªå¸¸ç”¨çš„ç½‘ç«™æŠ½å–æˆå•ç‹¬çš„ç±»ï¼Œä½œä¸º `ArticleCrawler` çš„å­ç±»ã€‚

å…¶ä¸­éœ€è¦æ”¹å˜çš„æ–¹æ³•ä¸º `__init__` ä¸ `parse_detail`ï¼Œå°† `tag / class_ / id` å±æ€§å†™æ­»ï¼Œä¸éœ€è¦äººä¸ºæŒ‡å®šã€‚

## å‘½ä»¤æ–¹å¼è¿è¡Œ

æˆ‘ä»¬é€šè¿‡å‘½ä»¤çš„æ–¹å¼ä½¿ç”¨è¯¥å·¥å…·ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æŒ‡å®šä¸€ä¸ªç¨‹åºå…¥å£ `__main__` æ–‡ä»¶ï¼š

  * æˆ‘ä»¬é€šè¿‡ OptionParserï¼ŒæŒ‡å®šå‘½ä»¤å‚æ•°è¯¦æƒ…ï¼Œå…¶ä¸­åŒ…å«åŒ…æè¿°ã€ç‰ˆæœ¬å·ã€å‚æ•°ç®€å†™ã€å‚æ•°åã€å¸®åŠ©æ‰‹å†Œç­‰ä¿¡æ¯ã€‚


    
    
    ```python
    if __name__ == '__main__':
     Â  Â from optparse import OptionParser
    â€‹
     Â  Â parser = OptionParser(prog=prog, description=description, version='%prog ' + version, usage=usage)
     Â  Â parser.add_option("-u", "--url", dest="url", help="crawled url (required)")
     Â  Â parser.add_option("-t", "--type", dest="type", default="",
     Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â help="crawled article type [csdn] | [juejin] | [zhihu] | [jianshu]")
     Â  Â parser.add_option("-o", "--output_folder", dest="output_folder",
     Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â help="output html / markdown / pdf folder (required)")
     Â  Â parser.add_option("-w", "--website_tag", dest="website_tag",
     Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â help="position of the article content in HTML (not required if 'type' is specified)")
     Â  Â parser.add_option("-c", "--class", dest="class_", default="",
     Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â help="position of the article content in HTML (not required if 'type' is specified)")
     Â  Â parser.add_option("-i", "--id", dest="id", default="",
     Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â help="position of the article content in HTML (not required if 'type' is specified)")
     Â  Â options, args = parser.parse_args()
     Â  Â main()
    
    ```

  * è¿›å…¥`main` æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¾æ®ä»£ç é€»è¾‘ï¼Œå¯¹å‚æ•°è¿›è¡Œé¢å¤–æ ¡éªŒï¼Œå¦‚ï¼šç©ºå‚æ•°å¼‚å¸¸ã€å‚æ•°é”™è¯¯å¼‚å¸¸ç­‰

    * `url` ä¸ `output_folder` ä¸å¾—ä¸ºç©º
    * `type / website_tag / class_ / id` ä¸å¾—åŒæ—¶ä¸ºç©º
    * `type` å¿…é¡»åœ¨æŒ‡å®šçš„ç±»å‹å†…
    * å‚æ•°æ ¡éªŒå®Œæ¯•åï¼Œåˆ›å»ºå¯¹åº”çš„ç±»å¯¹è±¡ï¼Œå¹¶æ‰§è¡Œ `start` æ–¹æ³•


    
    
    ```python
    def main():
     Â  Â url = options.url
     Â  Â type = options.type
     Â  Â output_folder = options.output_folder
     Â  Â website_tag = options.website_tag
     Â  Â class_ = options.class_
     Â  Â id = options.id
     Â  Â if not url:
     Â  Â  Â  Â parser.error("url must be specified.")
     Â  Â if not output_folder:
     Â  Â  Â  Â parser.error("output folder must be specified.")
     Â  Â if type == "" and website_tag == "" and class_ == "" and id == "":
     Â  Â  Â  Â parser.error("'type', 'website_tag', 'class_', 'id' cannot be empty at the same time.")
     Â  Â if type not in ["csdn", "juejin", "zhihu", "jianshu"]:
     Â  Â  Â  Â parser.error(
     Â  Â  Â  Â  Â  Â "The current article type is not supported, you need to specify 'class_' or 'id' to locate the position of the article.")
     Â  Â if type != '':
     Â  Â  Â  Â crawler = class_dic[type](url=url, output_folder=output_folder)
     Â  Â else:
     Â  Â  Â  Â crawler = ArticleCrawler(url=url, output_folder=output_folder, tag=website_tag, class_=class_, id=id)
     Â  Â crawler.start()
    
    ```

## æœ€ç»ˆæ•ˆæœ

æœ€ç»ˆï¼Œæˆ‘ä»¬å°†å…¶æ‰“åŒ…å‘å¸ƒåˆ° [pypi](https://link.juejin.cn?target=https%3A%2F%2Fpypi.org%2Fproject%2Farticle-crawler%2F "https://pypi.org/project/article-crawler/") ä¸­ï¼Œå¹¶é‡æ–°å®‰è£…åˆ°æœ¬åœ°ï¼Œæ‰§è¡Œå‘½ä»¤ï¼š
    
    
    ```bash
    pip install article-crawler
    python3 -m article_crawler -u https://zhuanlan.zhihu.com/p/644525159 -o /Users/lty/Downloads/article_output -t zhihu
    
    ```

å…¶å®ç°æ•ˆæœå¦‚ä¸‹ï¼š

![image-20230805211558806](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c87dc5f74690437691ee1ca800ea86ad~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) 

æˆ‘ä»¬æ‰“å¼€è¾“å‡ºçš„ Markdown æ–‡ä»¶ï¼Œçœ‹çœ‹æ•ˆæœï¼š

![image-20230805211708640](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5add477c7149424894f93e6b010d1d84~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) 

å¤§å®¶å¯ä»¥çœ‹åˆ°ï¼Œé™¤äº†æ¢è¡Œé—®é¢˜å¤–ï¼Œå…¶å®ƒéƒ¨åˆ†çš„è½¬æ¢æ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„ï¼ŒåŸºæœ¬ä¸åŸæ–‡ä¸€è‡´ï½

## æ€»ç»“

ä»Šå¤©ï¼Œæˆ‘ä»éœ€æ±‚åˆ†æã€æŠ€æœ¯é€‰å‹ã€å®ç°æ–¹æ¡ˆã€ä¼˜åŒ–ã€æ•ˆæœå±•ç¤ºç­‰è§’åº¦ï¼Œä» 0 åˆ° 1 å®ç°äº† Article Crawler å·¥å…·ï¼Œå¹¶å‘å¤§å®¶ä»‹ç»äº†è¯¦ç»†çš„å®ç°è¿‡ç¨‹ã€‚

ä»£ç å’ŒåŒ…å·²ç»å¼€æºï¼Œå¤§å®¶æ„Ÿå…´è¶£çš„å¯ä»¥å»ä½¿ç”¨ä¸€ä¸‹ï¼Œå¦‚æœæœ‰é—®é¢˜çš„è¯ï¼Œéº»çƒ¦æä¸€ä¸‹ Issue å‘€ï½

åœ°å€å¦‚ä¸‹ï¼š

  * Github åœ°å€ï¼š[github.com/ltyzzzxxx/aâ€¦](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fltyzzzxxx%2Farticle_crawler "https://github.com/ltyzzzxxx/article_crawler")
  * PyPi åœ°å€ï¼š[pypi.org/project/artâ€¦](https://link.juejin.cn?target=https%3A%2F%2Fpypi.org%2Fproject%2Farticle-crawler%2F "https://pypi.org/project/article-crawler/")



å¯¹äºå¦‚ä½•ä» 0 åˆ° 1 å‘å¸ƒä¸€ä¸ª Pypi åŒ…ï¼Œæˆ‘ä¼šå†ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œè¯¦ç»†è¿›è¡Œä»‹ç»ï½

ä»Šå¤©çš„å†…å®¹å°±åˆ°è¿™é‡Œå•¦ï¼Œå¤§å®¶è§‰å¾—æœ‰ç”¨çš„è¯éº»çƒ¦å¸®å¿™ç‚¹ä¸ªèµã€ç‚¹ä¸ª Star æ”¯æŒä¸€ä¸‹å‘€ï¼Œä¸‹æœŸå†è§ï¼

:::tip ç‰ˆæƒè¯´æ˜

ä½œè€…ï¼šå‘¨ä¸‰ä¸Coding

é“¾æ¥ï¼šhttps://juejin.cn/post/7263840667826323493
::: 
